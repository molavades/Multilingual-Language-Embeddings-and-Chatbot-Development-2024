{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929c3413-a858-4373-a115-dae2d919ec28",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59e882-204d-4c88-9ec8-c61fbb96baf4",
   "metadata": {},
   "source": [
    "**Train Language Embeddings: Use the provided Word2Vec notebook to train embeddings in a language other than English (your own language).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b5f3b-a6c9-459e-a5ed-9ae573c6f9f7",
   "metadata": {},
   "source": [
    "Using indicnlp library, for natural language processing in Indian languages (Hindi in our case). We are using indicnlp as it provides better tools and resources for working with various Indian languages, including tokenization, trasliteration, and other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a790590-7048-4b8e-af7c-10561c38f920",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affa5def-dad8-40aa-9edc-01ae70db4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc9ecc-e772-4725-ba14-5b5749c8ece8",
   "metadata": {},
   "source": [
    "## 1 Train Language Embeddings with Word2Vec (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9ffceb3d-03b5-440d-80f4-beb3da8e5a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Hindi text data\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindipoems.txt', 'r', encoding='utf-8') as file:\n",
    "    hindi_text = file.read()\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sentence_tokenize.sentence_split(hindi_text, lang='hi')\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenized_sentences = [indic_tokenize.trivial_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec CBOW model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0)  # sg=0 for CBOW\n",
    "\n",
    "# Save the model\n",
    "model.save(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_word2vec_cbow.model\")\n",
    "\n",
    "# Save the embeddings\n",
    "word_vectors = model.wv\n",
    "word_vectors.save(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_word2vec_cbow_vectors.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5d939af9-8d22-47f9-809f-1ec9e691e7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0.37085211277008057), ('शिशु\\nउनमें', 0.35214439034461975), ('का', 0.3512568473815918), ('मत', 0.34760239720344543), ('\\nजीवन', 0.34136736392974854), ('था', 0.341358482837677), ('करता', 0.3359297513961792), ('\\nमेरी', 0.33115971088409424), ('उदय', 0.3304550051689148), ('पर', 0.32900917530059814)]\n"
     ]
    }
   ],
   "source": [
    "# Example of using the model\n",
    "print(model.wv.most_similar(\"प्यार\"))  # Replace with any Hindi word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f3d42-1d50-4b02-a1f4-143bf0cc1142",
   "metadata": {},
   "source": [
    "## 2 Develop RNN-based Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0cf6e99c-c03b-434f-8c19-2ced128d2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7555712-c219-4a14-8af0-e61713e5b691",
   "metadata": {},
   "source": [
    "**First, let's create default Keras embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b15f3724-ff07-408e-aca2-b86cc68af951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">160,200</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │         \u001b[38;5;34m160,200\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,200</span> (625.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,200\u001b[0m (625.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,200</span> (625.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,200\u001b[0m (625.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Keras embeddings saved in text format.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Assuming you've already processed your text data and created tokenized_sentences\n",
    "\n",
    "# Flatten the list of tokenized sentences\n",
    "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(all_words)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create default Keras embedding model\n",
    "embedding_dim = 100\n",
    "keras_embedding_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "])\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "keras_embedding_model.build((None, max_length))\n",
    "\n",
    "# Compile the model\n",
    "keras_embedding_model.compile('adam', 'mse')\n",
    "\n",
    "# Print model summary\n",
    "print(keras_embedding_model.summary())\n",
    "\n",
    "# Get the default Keras embeddings\n",
    "keras_embeddings = keras_embedding_model.layers[0].get_weights()[0]\n",
    "\n",
    "# Save the Keras embeddings in txt format\n",
    "with open(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_keras_embeddings5.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        vector = keras_embeddings[index]\n",
    "        vector_str = \" \".join([str(v) for v in vector])\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n",
    "\n",
    "print(\"Keras embeddings saved in text format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bda5d8-afc5-4372-b68d-ee26f93e1f8c",
   "metadata": {},
   "source": [
    "### Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "637c1479-d511-4b1e-b415-5e3b7a1a5968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the embeddings: 1601\n",
      "Embedding dimension: 100\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = [float(x) for x in values[1:]]\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load the saved embeddings\n",
    "loaded_embeddings = load_embeddings(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_keras_embeddings5.txt\")\n",
    "\n",
    "# Now you can use the loaded embeddings\n",
    "print(f\"Number of words in the embeddings: {len(loaded_embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(next(iter(loaded_embeddings.values())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe61bf1-e513-4e4b-a96c-7222a4c830d8",
   "metadata": {},
   "source": [
    "**Now, let's create RNN-based embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f73e2a2-6550-4d29-81ee-c30333c936d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">475,648</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │         \u001b[38;5;34m475,648\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_12 (\u001b[38;5;33mBidirectional\u001b[0m)     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">475,648</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m475,648\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">475,648</span> (1.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m475,648\u001b[0m (1.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize and prepare input-output pairs for the RNN\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(tokenized_sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences to ensure consistent input length\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Initialize random embedding matrix (replace with actual embeddings if available)\n",
    "embedding_dim = 256\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "# Define the RNN model with Bidirectional LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare input (X_train) and target (y_train) data for sequence prediction\n",
    "X_train = padded_sequences[:, :-1]  # All words except the last one in each sequence\n",
    "y_train = padded_sequences[:, 1:]   # All words except the first one in each sequence\n",
    "\n",
    "# Ensure y_train has the same shape as model output\n",
    "y_train = np.expand_dims(y_train, -1)\n",
    "\n",
    "# Print model summary to verify parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c084341-b786-4e2f-aea2-314747695b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 627ms/step - accuracy: 0.4577 - loss: 7.0337\n",
      "Epoch 2/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 594ms/step - accuracy: 0.8871 - loss: 4.5493\n",
      "Epoch 3/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 612ms/step - accuracy: 0.8851 - loss: 2.7010\n",
      "Epoch 4/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 605ms/step - accuracy: 0.8864 - loss: 1.4008\n",
      "Epoch 5/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 603ms/step - accuracy: 0.8838 - loss: 1.1095\n",
      "Epoch 6/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 581ms/step - accuracy: 0.8843 - loss: 1.1177\n",
      "Epoch 7/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 625ms/step - accuracy: 0.8856 - loss: 1.1150\n",
      "Epoch 8/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 590ms/step - accuracy: 0.8859 - loss: 1.1023\n",
      "Epoch 9/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.8861 - loss: 1.0792\n",
      "Epoch 10/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614ms/step - accuracy: 0.8824 - loss: 1.0991\n",
      "Epoch 11/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614ms/step - accuracy: 0.8808 - loss: 1.0884\n",
      "Epoch 12/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 661ms/step - accuracy: 0.8835 - loss: 1.0312\n",
      "Epoch 13/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 580ms/step - accuracy: 0.8861 - loss: 0.9774\n",
      "Epoch 14/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 596ms/step - accuracy: 0.8934 - loss: 0.8819\n",
      "Epoch 15/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 692ms/step - accuracy: 0.8822 - loss: 0.9453\n",
      "Epoch 16/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 725ms/step - accuracy: 0.8855 - loss: 0.8889\n",
      "Epoch 17/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 846ms/step - accuracy: 0.8838 - loss: 0.8654\n",
      "Epoch 18/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 706ms/step - accuracy: 0.8847 - loss: 0.8337\n",
      "Epoch 19/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 718ms/step - accuracy: 0.8863 - loss: 0.7975\n",
      "Epoch 20/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 744ms/step - accuracy: 0.8933 - loss: 0.7416\n",
      "Epoch 21/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 609ms/step - accuracy: 0.8759 - loss: 0.8440\n",
      "Epoch 22/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 759ms/step - accuracy: 0.8819 - loss: 0.7922\n",
      "Epoch 23/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 714ms/step - accuracy: 0.8838 - loss: 0.7682\n",
      "Epoch 24/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 628ms/step - accuracy: 0.8848 - loss: 0.7545\n",
      "Epoch 25/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 602ms/step - accuracy: 0.8814 - loss: 0.7791\n",
      "Epoch 26/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 596ms/step - accuracy: 0.8895 - loss: 0.7657\n",
      "Epoch 27/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 681ms/step - accuracy: 0.8933 - loss: 0.7590\n",
      "Epoch 28/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 597ms/step - accuracy: 0.8915 - loss: 0.7747\n",
      "Epoch 29/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629ms/step - accuracy: 0.8970 - loss: 0.7305\n",
      "Epoch 30/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 591ms/step - accuracy: 0.8956 - loss: 0.7386\n",
      "Epoch 31/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 603ms/step - accuracy: 0.8963 - loss: 0.7317\n",
      "Epoch 32/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 632ms/step - accuracy: 0.8978 - loss: 0.7204\n",
      "Epoch 33/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 580ms/step - accuracy: 0.9005 - loss: 0.6967\n",
      "Epoch 34/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 622ms/step - accuracy: 0.9003 - loss: 0.6967\n",
      "Epoch 35/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - accuracy: 0.8973 - loss: 0.7184\n",
      "Epoch 36/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.8949 - loss: 0.7358\n",
      "Epoch 37/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.8983 - loss: 0.7052\n",
      "Epoch 38/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 678ms/step - accuracy: 0.8961 - loss: 0.7205\n",
      "Epoch 39/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 603ms/step - accuracy: 0.9000 - loss: 0.6932\n",
      "Epoch 40/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 626ms/step - accuracy: 0.8941 - loss: 0.7326\n",
      "Epoch 41/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 604ms/step - accuracy: 0.8985 - loss: 0.6978\n",
      "Epoch 42/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 645ms/step - accuracy: 0.8976 - loss: 0.7032\n",
      "Epoch 43/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599ms/step - accuracy: 0.8969 - loss: 0.7057\n",
      "Epoch 44/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 640ms/step - accuracy: 0.8975 - loss: 0.7018\n",
      "Epoch 45/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 610ms/step - accuracy: 0.8947 - loss: 0.7183\n",
      "Epoch 46/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 790ms/step - accuracy: 0.8924 - loss: 0.7333\n",
      "Epoch 47/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.8950 - loss: 0.7130\n",
      "Epoch 48/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 643ms/step - accuracy: 0.8983 - loss: 0.6898\n",
      "Epoch 49/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - accuracy: 0.9031 - loss: 0.6533\n",
      "Epoch 50/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 613ms/step - accuracy: 0.8952 - loss: 0.7054\n",
      "Epoch 51/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - accuracy: 0.8977 - loss: 0.6881\n",
      "Epoch 52/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 610ms/step - accuracy: 0.8940 - loss: 0.7116\n",
      "Epoch 53/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 739ms/step - accuracy: 0.8888 - loss: 0.7478\n",
      "Epoch 54/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 612ms/step - accuracy: 0.8981 - loss: 0.6829\n",
      "Epoch 55/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 624ms/step - accuracy: 0.8970 - loss: 0.6864\n",
      "Epoch 56/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 642ms/step - accuracy: 0.8982 - loss: 0.6805\n",
      "Epoch 57/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 650ms/step - accuracy: 0.9044 - loss: 0.6320\n",
      "Epoch 58/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 768ms/step - accuracy: 0.9022 - loss: 0.6454\n",
      "Epoch 59/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 748ms/step - accuracy: 0.9023 - loss: 0.6439\n",
      "Epoch 60/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 776ms/step - accuracy: 0.8953 - loss: 0.6891\n",
      "Epoch 61/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 716ms/step - accuracy: 0.8994 - loss: 0.6616\n",
      "Epoch 62/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 590ms/step - accuracy: 0.8991 - loss: 0.6664\n",
      "Epoch 63/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595ms/step - accuracy: 0.9000 - loss: 0.6554\n",
      "Epoch 64/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 633ms/step - accuracy: 0.8987 - loss: 0.6627\n",
      "Epoch 65/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 635ms/step - accuracy: 0.8925 - loss: 0.7027\n",
      "Epoch 66/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 686ms/step - accuracy: 0.8982 - loss: 0.6637\n",
      "Epoch 67/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 609ms/step - accuracy: 0.8955 - loss: 0.6828\n",
      "Epoch 68/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629ms/step - accuracy: 0.8985 - loss: 0.6577\n",
      "Epoch 69/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 677ms/step - accuracy: 0.8972 - loss: 0.6667\n",
      "Epoch 70/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 607ms/step - accuracy: 0.8960 - loss: 0.6718\n",
      "Epoch 71/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 745ms/step - accuracy: 0.9005 - loss: 0.6405\n",
      "Epoch 72/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 706ms/step - accuracy: 0.8986 - loss: 0.6531\n",
      "Epoch 73/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 671ms/step - accuracy: 0.9005 - loss: 0.6380\n",
      "Epoch 74/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599ms/step - accuracy: 0.8937 - loss: 0.6836\n",
      "Epoch 75/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 717ms/step - accuracy: 0.8961 - loss: 0.6644\n",
      "Epoch 76/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 661ms/step - accuracy: 0.9020 - loss: 0.6236\n",
      "Epoch 77/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.8996 - loss: 0.6366\n",
      "Epoch 78/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - accuracy: 0.9009 - loss: 0.6270\n",
      "Epoch 79/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 670ms/step - accuracy: 0.8981 - loss: 0.6426\n",
      "Epoch 80/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 659ms/step - accuracy: 0.8933 - loss: 0.6714\n",
      "Epoch 81/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 615ms/step - accuracy: 0.9016 - loss: 0.6157\n",
      "Epoch 82/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 620ms/step - accuracy: 0.8984 - loss: 0.6361\n",
      "Epoch 83/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 651ms/step - accuracy: 0.9019 - loss: 0.6155\n",
      "Epoch 84/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 631ms/step - accuracy: 0.9008 - loss: 0.6154\n",
      "Epoch 85/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.9030 - loss: 0.6050\n",
      "Epoch 86/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 637ms/step - accuracy: 0.9029 - loss: 0.6023\n",
      "Epoch 87/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 608ms/step - accuracy: 0.8985 - loss: 0.6277\n",
      "Epoch 88/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 682ms/step - accuracy: 0.8985 - loss: 0.6263\n",
      "Epoch 89/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 649ms/step - accuracy: 0.9022 - loss: 0.5972\n",
      "Epoch 90/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 724ms/step - accuracy: 0.9009 - loss: 0.6070\n",
      "Epoch 91/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 624ms/step - accuracy: 0.8982 - loss: 0.6202\n",
      "Epoch 92/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 638ms/step - accuracy: 0.9062 - loss: 0.5699\n",
      "Epoch 93/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 739ms/step - accuracy: 0.9043 - loss: 0.5767\n",
      "Epoch 94/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 632ms/step - accuracy: 0.8994 - loss: 0.6110\n",
      "Epoch 95/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 705ms/step - accuracy: 0.9038 - loss: 0.5853\n",
      "Epoch 96/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 666ms/step - accuracy: 0.8974 - loss: 0.6181\n",
      "Epoch 97/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 692ms/step - accuracy: 0.9092 - loss: 0.5426\n",
      "Epoch 98/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647ms/step - accuracy: 0.9030 - loss: 0.5800\n",
      "Epoch 99/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 623ms/step - accuracy: 0.8945 - loss: 0.6297\n",
      "Epoch 100/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 628ms/step - accuracy: 0.9027 - loss: 0.5769\n",
      "Epoch 101/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 700ms/step - accuracy: 0.9052 - loss: 0.5577\n",
      "Epoch 102/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 641ms/step - accuracy: 0.9033 - loss: 0.5694\n",
      "Epoch 103/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 602ms/step - accuracy: 0.9011 - loss: 0.5842\n",
      "Epoch 104/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 750ms/step - accuracy: 0.9017 - loss: 0.5772\n",
      "Epoch 105/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 711ms/step - accuracy: 0.8986 - loss: 0.5928\n",
      "Epoch 106/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 692ms/step - accuracy: 0.8994 - loss: 0.5863\n",
      "Epoch 107/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 616ms/step - accuracy: 0.9043 - loss: 0.5534\n",
      "Epoch 108/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 635ms/step - accuracy: 0.8981 - loss: 0.5958\n",
      "Epoch 109/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 692ms/step - accuracy: 0.9047 - loss: 0.5523\n",
      "Epoch 110/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 638ms/step - accuracy: 0.9056 - loss: 0.5529\n",
      "Epoch 111/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 623ms/step - accuracy: 0.9078 - loss: 0.5308\n",
      "Epoch 112/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 609ms/step - accuracy: 0.9023 - loss: 0.5626\n",
      "Epoch 113/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 593ms/step - accuracy: 0.9056 - loss: 0.5444\n",
      "Epoch 114/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 580ms/step - accuracy: 0.9051 - loss: 0.5521\n",
      "Epoch 115/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 696ms/step - accuracy: 0.9065 - loss: 0.5373\n",
      "Epoch 116/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 689ms/step - accuracy: 0.9096 - loss: 0.5208\n",
      "Epoch 117/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 603ms/step - accuracy: 0.9119 - loss: 0.5045\n",
      "Epoch 118/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 607ms/step - accuracy: 0.9059 - loss: 0.5355\n",
      "Epoch 119/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.9070 - loss: 0.5306\n",
      "Epoch 120/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 594ms/step - accuracy: 0.9106 - loss: 0.5101\n",
      "Epoch 121/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 589ms/step - accuracy: 0.9146 - loss: 0.4825\n",
      "Epoch 122/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 677ms/step - accuracy: 0.9122 - loss: 0.4968\n",
      "Epoch 123/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 657ms/step - accuracy: 0.9103 - loss: 0.5099\n",
      "Epoch 124/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 714ms/step - accuracy: 0.9116 - loss: 0.5115\n",
      "Epoch 125/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647ms/step - accuracy: 0.9136 - loss: 0.4933\n",
      "Epoch 126/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 611ms/step - accuracy: 0.9133 - loss: 0.5005\n",
      "Epoch 127/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 602ms/step - accuracy: 0.9113 - loss: 0.5091\n",
      "Epoch 128/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 634ms/step - accuracy: 0.9207 - loss: 0.4518\n",
      "Epoch 129/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 694ms/step - accuracy: 0.9184 - loss: 0.4660\n",
      "Epoch 130/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 585ms/step - accuracy: 0.9170 - loss: 0.4752\n",
      "Epoch 131/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 613ms/step - accuracy: 0.9148 - loss: 0.4873\n",
      "Epoch 132/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 596ms/step - accuracy: 0.9145 - loss: 0.4836\n",
      "Epoch 133/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617ms/step - accuracy: 0.9107 - loss: 0.5074\n",
      "Epoch 134/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 668ms/step - accuracy: 0.9179 - loss: 0.4651\n",
      "Epoch 135/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.9163 - loss: 0.4724\n",
      "Epoch 136/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599ms/step - accuracy: 0.9189 - loss: 0.4618\n",
      "Epoch 137/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - accuracy: 0.9200 - loss: 0.4544\n",
      "Epoch 138/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 604ms/step - accuracy: 0.9245 - loss: 0.4254\n",
      "Epoch 139/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 607ms/step - accuracy: 0.9260 - loss: 0.4185\n",
      "Epoch 140/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 665ms/step - accuracy: 0.9194 - loss: 0.4510\n",
      "Epoch 141/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 676ms/step - accuracy: 0.9228 - loss: 0.4327\n",
      "Epoch 142/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614ms/step - accuracy: 0.9196 - loss: 0.4487\n",
      "Epoch 143/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 650ms/step - accuracy: 0.9249 - loss: 0.4201\n",
      "Epoch 144/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 598ms/step - accuracy: 0.9200 - loss: 0.4374\n",
      "Epoch 145/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 602ms/step - accuracy: 0.9166 - loss: 0.4608\n",
      "Epoch 146/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599ms/step - accuracy: 0.9225 - loss: 0.4248\n",
      "Epoch 147/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 591ms/step - accuracy: 0.9238 - loss: 0.4180\n",
      "Epoch 148/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 603ms/step - accuracy: 0.9241 - loss: 0.4156\n",
      "Epoch 149/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595ms/step - accuracy: 0.9234 - loss: 0.4175\n",
      "Epoch 150/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 611ms/step - accuracy: 0.9258 - loss: 0.4022\n",
      "Epoch 151/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 624ms/step - accuracy: 0.9258 - loss: 0.4035\n",
      "Epoch 152/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 679ms/step - accuracy: 0.9255 - loss: 0.4044\n",
      "Epoch 153/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 602ms/step - accuracy: 0.9222 - loss: 0.4181\n",
      "Epoch 154/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617ms/step - accuracy: 0.9243 - loss: 0.4114\n",
      "Epoch 155/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 592ms/step - accuracy: 0.9237 - loss: 0.4198\n",
      "Epoch 156/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 631ms/step - accuracy: 0.9293 - loss: 0.3800\n",
      "Epoch 157/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 615ms/step - accuracy: 0.9301 - loss: 0.3759\n",
      "Epoch 158/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614ms/step - accuracy: 0.9281 - loss: 0.3827\n",
      "Epoch 159/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 689ms/step - accuracy: 0.9286 - loss: 0.3801\n",
      "Epoch 160/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 765ms/step - accuracy: 0.9314 - loss: 0.3640\n",
      "Epoch 161/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752ms/step - accuracy: 0.9330 - loss: 0.3577\n",
      "Epoch 162/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 646ms/step - accuracy: 0.9272 - loss: 0.3869\n",
      "Epoch 163/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 681ms/step - accuracy: 0.9327 - loss: 0.3526\n",
      "Epoch 164/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 604ms/step - accuracy: 0.9328 - loss: 0.3527\n",
      "Epoch 165/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647ms/step - accuracy: 0.9295 - loss: 0.3690\n",
      "Epoch 166/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 746ms/step - accuracy: 0.9325 - loss: 0.3538\n",
      "Epoch 167/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 807ms/step - accuracy: 0.9340 - loss: 0.3466\n",
      "Epoch 168/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 670ms/step - accuracy: 0.9352 - loss: 0.3408\n",
      "Epoch 169/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617ms/step - accuracy: 0.9375 - loss: 0.3249\n",
      "Epoch 170/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 635ms/step - accuracy: 0.9348 - loss: 0.3395\n",
      "Epoch 171/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614ms/step - accuracy: 0.9357 - loss: 0.3341\n",
      "Epoch 172/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 679ms/step - accuracy: 0.9360 - loss: 0.3345\n",
      "Epoch 173/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 624ms/step - accuracy: 0.9394 - loss: 0.3190\n",
      "Epoch 174/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 649ms/step - accuracy: 0.9367 - loss: 0.3264\n",
      "Epoch 175/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 651ms/step - accuracy: 0.9384 - loss: 0.3184\n",
      "Epoch 176/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 597ms/step - accuracy: 0.9416 - loss: 0.3072\n",
      "Epoch 177/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 627ms/step - accuracy: 0.9359 - loss: 0.3326\n",
      "Epoch 178/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 643ms/step - accuracy: 0.9406 - loss: 0.3085\n",
      "Epoch 179/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 656ms/step - accuracy: 0.9379 - loss: 0.3212\n",
      "Epoch 180/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629ms/step - accuracy: 0.9447 - loss: 0.2874\n",
      "Epoch 181/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 581ms/step - accuracy: 0.9426 - loss: 0.2983\n",
      "Epoch 182/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647ms/step - accuracy: 0.9433 - loss: 0.2905\n",
      "Epoch 183/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600ms/step - accuracy: 0.9379 - loss: 0.3186\n",
      "Epoch 184/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 688ms/step - accuracy: 0.9438 - loss: 0.2911\n",
      "Epoch 185/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 672ms/step - accuracy: 0.9411 - loss: 0.3055\n",
      "Epoch 186/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 693ms/step - accuracy: 0.9487 - loss: 0.2634\n",
      "Epoch 187/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 682ms/step - accuracy: 0.9483 - loss: 0.2718\n",
      "Epoch 188/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 643ms/step - accuracy: 0.9466 - loss: 0.2778\n",
      "Epoch 189/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 695ms/step - accuracy: 0.9481 - loss: 0.2706\n",
      "Epoch 190/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 715ms/step - accuracy: 0.9510 - loss: 0.2576\n",
      "Epoch 191/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 633ms/step - accuracy: 0.9523 - loss: 0.2507\n",
      "Epoch 192/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 663ms/step - accuracy: 0.9507 - loss: 0.2593\n",
      "Epoch 193/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 720ms/step - accuracy: 0.9510 - loss: 0.2620\n",
      "Epoch 194/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 661ms/step - accuracy: 0.9484 - loss: 0.2750\n",
      "Epoch 195/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 691ms/step - accuracy: 0.9553 - loss: 0.2428\n",
      "Epoch 196/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 707ms/step - accuracy: 0.9533 - loss: 0.2530\n",
      "Epoch 197/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 658ms/step - accuracy: 0.9562 - loss: 0.2414\n",
      "Epoch 198/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 724ms/step - accuracy: 0.9557 - loss: 0.2385\n",
      "Epoch 199/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 619ms/step - accuracy: 0.9564 - loss: 0.2374\n",
      "Epoch 200/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 697ms/step - accuracy: 0.9584 - loss: 0.2322\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1dc6066a-f4f6-43b8-80be-b6c3c5985e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN embeddings saved in text format.\n"
     ]
    }
   ],
   "source": [
    "# Extract the trained embeddings\n",
    "trained_embeddings = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Save the embeddings in txt format\n",
    "with open(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_rnn_embeddings5.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        vector = trained_embeddings[index]\n",
    "        vector_str = \" \".join([str(v) for v in vector])\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n",
    "\n",
    "print(\"RNN embeddings saved in text format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd188698-8335-4395-abcf-64921bcbfc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'प्यार' using RNN embeddings:\n",
      "लाना: 0.7911\n",
      "आ‌ई: 0.7882\n",
      "पांव: 0.7870\n",
      "स्नेह: 0.7864\n",
      "मूँदे: 0.7838\n"
     ]
    }
   ],
   "source": [
    "# Function to find similar words using embeddings\n",
    "def find_similar_words(word, embeddings, word_index, top_n=5):\n",
    "    if word not in word_index:\n",
    "        return []\n",
    "    \n",
    "    word_vector = embeddings[word_index[word]]\n",
    "    similarities = []\n",
    "    \n",
    "    for w, i in word_index.items():\n",
    "        if w != word:\n",
    "            similarity = np.dot(word_vector, embeddings[i]) / (np.linalg.norm(word_vector) * np.linalg.norm(embeddings[i]))\n",
    "            similarities.append((w, similarity))\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# Demonstrate RNN embedding quality\n",
    "test_word = \"प्यार\"  # Replace with a Hindi word from your vocabulary\n",
    "similar_words = find_similar_words(test_word, rnn_embeddings, tokenizer.word_index)\n",
    "print(f\"Words similar to '{test_word}' using RNN embeddings:\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4bc92609-d901-40dd-b512-321b7893a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Similarity Comparison:\n",
      "\n",
      "Similar words to 'प्यार':\n",
      "Keras embeddings:\n",
      "  इन्द्रधनुष: 0.3738\n",
      "  बनाना: 0.3259\n",
      "  हमारी: 0.2849\n",
      "  कवि: 0.2752\n",
      "  कल: 0.2727\n",
      "RNN embeddings:\n",
      "  बजता: 0.7982\n",
      "  .: 0.7964\n",
      "  तरफ: 0.7921\n",
      "  बिखराता: 0.7875\n",
      "  उखड़: 0.7861\n",
      "\n",
      "Similar words to 'जीवन':\n",
      "Keras embeddings:\n",
      "  सजीव: 0.3683\n",
      "  अर्पण: 0.3386\n",
      "  वितरण: 0.3153\n",
      "  मंदिर: 0.3120\n",
      "  भेद: 0.3018\n",
      "RNN embeddings:\n",
      "  सरित: 0.8102\n",
      "  छूट: 0.8037\n",
      "  लहर: 0.8033\n",
      "  पक्‍के: 0.7998\n",
      "  ’: 0.7995\n",
      "\n",
      "Similar words to 'समय':\n",
      "Keras embeddings:\n",
      "  मुझमें: 0.2986\n",
      "  लगा: 0.2959\n",
      "  रात्रि: 0.2787\n",
      "  रथ: 0.2729\n",
      "  कलिकाएँ: 0.2720\n",
      "RNN embeddings:\n",
      "  गहरे: 0.7935\n",
      "  उठा: 0.7915\n",
      "  चुस्त: 0.7910\n",
      "  छिपा: 0.7898\n",
      "  बोली: 0.7897\n",
      "\n",
      "Similar words to 'सुंदर':\n",
      "Keras embeddings:\n",
      "  नीले: 0.3294\n",
      "  ज्योति: 0.3243\n",
      "  उगले: 0.3232\n",
      "  संतान: 0.3220\n",
      "  नाचूँगा: 0.3100\n",
      "RNN embeddings:\n",
      "  मेहनत: 0.8117\n",
      "  वक़्त: 0.8116\n",
      "  फेनिल: 0.8084\n",
      "  नींद: 0.8081\n",
      "  लेकर: 0.8062\n",
      "\n",
      "Similar words to 'दुनिया':\n",
      "Keras embeddings:\n",
      "  उन्हे: 0.3416\n",
      "  जमुन: 0.2999\n",
      "  इसे: 0.2940\n",
      "  खड़ा: 0.2837\n",
      "  निकलता: 0.2763\n",
      "RNN embeddings:\n",
      "  सत्‍य: 0.7890\n",
      "  बड़ा: 0.7858\n",
      "  लज्जित: 0.7850\n",
      "  विजय: 0.7821\n",
      "  दूँगा: 0.7816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            values = line.split()\n",
    "            if len(values) < 2:  # Check if there's at least a word and one vector component\n",
    "                #print(f\"Warning: Skipping line {line_num} due to insufficient data: {line}\")\n",
    "                continue\n",
    "            try:\n",
    "                word = values[0]\n",
    "                vector = np.array([float(x) for x in values[1:]])\n",
    "                embeddings[word] = vector\n",
    "            except ValueError as e:\n",
    "                print(f\"Error on line {line_num}: {e}\")\n",
    "                print(f\"Problematic line: {line}\")\n",
    "                continue\n",
    "    return embeddings\n",
    "\n",
    "# Load the saved embeddings\n",
    "keras_embeddings = load_embeddings(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_keras_embeddings5.txt\")\n",
    "rnn_embeddings = load_embeddings(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_rnn_embeddings5.txt\")\n",
    "\n",
    "# Create word_index from the loaded embeddings\n",
    "word_index = {word: i for i, word in enumerate(keras_embeddings.keys())}\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "def find_similar_words(word, embeddings, top_n=5):\n",
    "    if word not in embeddings:\n",
    "        return []\n",
    "    \n",
    "    word_vector = embeddings[word]\n",
    "    similarities = []\n",
    "    \n",
    "    for w, vec in embeddings.items():\n",
    "        if w != word:\n",
    "            similarity = 1 - cosine(word_vector, vec)\n",
    "            similarities.append((w, similarity))\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# 1. Word similarity comparison\n",
    "test_words = [\"प्यार\", \"जीवन\", \"समय\", \"सुंदर\", \"दुनिया\"]  # Replace with Hindi words from your vocabulary\n",
    "\n",
    "print(\"Word Similarity Comparison:\")\n",
    "for word in test_words:\n",
    "    print(f\"\\nSimilar words to '{word}':\")\n",
    "    print(\"Keras embeddings:\")\n",
    "    keras_similar = find_similar_words(word, keras_embeddings)\n",
    "    for w, sim in keras_similar:\n",
    "        print(f\"  {w}: {sim:.4f}\")\n",
    "    \n",
    "    print(\"RNN embeddings:\")\n",
    "    rnn_similar = find_similar_words(word, rnn_embeddings)\n",
    "    for w, sim in rnn_similar:\n",
    "        print(f\"  {w}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12416071-bfec-49ea-9f4b-33aed3c83660",
   "metadata": {},
   "source": [
    "# 3 Build a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a83b5a-7d34-4f8b-bcb2-7e4c94c52735",
   "metadata": {},
   "source": [
    "**3. Build a Chatbot: Create a chatbot that uses the trained embeddings and evaluate its performance against a similar chatbot that uses English embeddings. Experiment with Pre-trained Embeddings: Optionally, download and integrate pre-trained embeddings for your language and compare their impact on the chatbot’s performance versus the embeddings you trained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab69973e-10de-41fd-bb15-b779c1b3f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a5972c0c-abe2-45a3-a53a-dfc49ea54caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bef4e-b9cb-49c7-8651-345bae6daf22",
   "metadata": {},
   "source": [
    "## Read, process, and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b430b5b3-1481-4728-b3e5-9e9109c761f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_numbers(text):\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ed1c97f-f0b6-4ede-9e54-2359edf6af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Heal the text\n",
    "text = ''\n",
    "print( \"Reading txt file...\")\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindipoems.txt',  'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# sentence delimiter processing\n",
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_  \")\n",
    "text = text.replace(\":\", \" _comma_  \")\n",
    "text = text.replace(\";\", \" _comma_  \")\n",
    "\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "\n",
    "text = text.replace('\"',\"\")\n",
    "\n",
    "# i leave apostrophes in place, spawning separate words\n",
    "#text = text.replace(\"’\",\"\")\n",
    "\n",
    "# absorb tabs\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "text = text.replace(\"  \", \"\")\n",
    "\n",
    "# remove numbes\n",
    "text = clean_numbers(text)\n",
    "\n",
    "# absorb soace\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "text = _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5e05ecd-88ff-4c58-b64f-f8330ed1502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "text = text.replace('i ', 'I ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be5d5665-6260-4c22-af29-7366451dd13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27139"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b3c12a3e-6ee3-476f-92c4-6b4365a6bc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poems by harivansh raI bachchan source<https _comma_//hindionlinejankari.com/harivanshraibachchanpoems/> & <https _comma_//hindikavita.com/hindipoetryharivanshraibachchan.php> •• अग्निपथ कविता •• वृक्ष हों भले खड़े _eol_ हों घने हों बड़े _eol_ एक पत्र छाँह भी _eol_ माँग मत _comma_ माँग मत _comma_ माँग मत _eol_ अग्निपथ अग्निपथ अग्निपथ। तू न थकेगा कभी _comma_ तू न रुकेगा कभी _eol_ तू न मुड़ेगा कभी _eol_ कर शपथ _comma_ कर शपथ _comma_ कर शपथ _eol_ अग्निपथ अग्निपथ अग्निपथ। यह महान दृश्य है _eol_ चल रहा मनुष्य है _eol_ अश्रु श्वेत रक्त से _eol_ लथपथ लथपथ लथपथ _eol_ अग्निपथ अग्निपथ अग्निपथ। ••• नीड़ का निर्माण ••• नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। वह उठी आँधी कि नभ में छा गया सहसा अँधेरा _eol_ धूलि धूसर बादलों ने भूमि को इस भाँति घेरा _eol_ रातसा दिन हो गया _comma_ फिर रात आ\\u200cई और काली _eol_ लग रहा था अब न होगा इस निशा का फिर सवेरा _eol_ रात के उत्पातभय से भीत जनजन _comma_ भीत कणकण किंतु प्राची से उषा की मोहिनी मुस्कान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। वह चले झोंके कि काँपे भीम कायावान भूधर _eol_ जड़ समेत उखड़पुखड़कर गिर पड़े _comma_ टूटे विटप वर _eol_ हाय _comma_ तिनकों से विनिर्मित घोंसलो पर क्या न बीती _eol_ डगमगा\\u200cए जबकि कंकड़ _eol_ ईंट _comma_ पत्थर के महलघर बोल आशा के विहंगम _eol_ किस जगह पर तू छिपा था _eol_ जो गगन पर चढ़ उठाता गर्व से निज तान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। क्रुद्ध नभ के वज्र दंतों में उषा है मुसकराती _eol_ घोर गर्जनमय गगन के कंठ में खग पंक्ति गाती _comma_ एक चिड़िया चोंच में तिनका लि\\u200cए जो जा रही है _eol_ वह सहज में ही पवन उंचास को नीचा दिखाती नाश के दुख से कभी दबता नहीं निर्माण का सुख प्रलय की निस्तब्धता से सृष्टि का नव गान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। •• पथ की पहचान •• पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले पुस्तकों में है नहीं छापी गई इसकी कहानी _eol_ हाल इसका ज्ञात होता है न औरों की जबानी _eol_ अनगिनत राही गए इस राह से _comma_ उनका पता क्या _eol_ पर गए कुछ लोग इस पर छोड़ पैरों की निशानी _eol_ यह निशानी मूक होकर भी बहुत कुछ बोलती है _eol_ खोल इसक'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bca9ed9d-8a60-4f77-9961-6c9ec0a2f120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poems by harivansh raI bachchan source<https _comma_//hindionlinejankari',\n",
       " 'com/harivanshraibachchanpoems/> & <https _comma_//hindikavita',\n",
       " 'com/hindipoetryharivanshraibachchan',\n",
       " 'php> •• अग्निपथ कविता •• वृक्ष हों भले खड़े _eol_ हों घने हों बड़े _eol_ एक पत्र छाँह भी _eol_ माँग मत _comma_ माँग मत _comma_ माँग मत _eol_ अग्निपथ अग्निपथ अग्निपथ। तू न थकेगा कभी _comma_ तू न रुकेगा कभी _eol_ तू न मुड़ेगा कभी _eol_ कर शपथ _comma_ कर शपथ _comma_ कर शपथ _eol_ अग्निपथ अग्निपथ अग्निपथ। यह महान दृश्य है _eol_ चल रहा मनुष्य है _eol_ अश्रु श्वेत रक्त से _eol_ लथपथ लथपथ लथपथ _eol_ अग्निपथ अग्निपथ अग्निपथ। ••• नीड़ का निर्माण ••• नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। वह उठी आँधी कि नभ में छा गया सहसा अँधेरा _eol_ धूलि धूसर बादलों ने भूमि को इस भाँति घेरा _eol_ रातसा दिन हो गया _comma_ फिर रात आ\\u200cई और काली _eol_ लग रहा था अब न होगा इस निशा का फिर सवेरा _eol_ रात के उत्पातभय से भीत जनजन _comma_ भीत कणकण किंतु प्राची से उषा की मोहिनी मुस्कान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। वह चले झोंके कि काँपे भीम कायावान भूधर _eol_ जड़ समेत उखड़पुखड़कर गिर पड़े _comma_ टूटे विटप वर _eol_ हाय _comma_ तिनकों से विनिर्मित घोंसलो पर क्या न बीती _eol_ डगमगा\\u200cए जबकि कंकड़ _eol_ ईंट _comma_ पत्थर के महलघर बोल आशा के विहंगम _eol_ किस जगह पर तू छिपा था _eol_ जो गगन पर चढ़ उठाता गर्व से निज तान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। क्रुद्ध नभ के वज्र दंतों में उषा है मुसकराती _eol_ घोर गर्जनमय गगन के कंठ में खग पंक्ति गाती _comma_ एक चिड़िया चोंच में तिनका लि\\u200cए जो जा रही है _eol_ वह सहज में ही पवन उंचास को नीचा दिखाती नाश के दुख से कभी दबता नहीं निर्माण का सुख प्रलय की निस्तब्धता से सृष्टि का नव गान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। •• पथ की पहचान •• पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले पुस्तकों में है नहीं छापी गई इसकी कहानी _eol_ हाल इसका ज्ञात होता है न औरों की जबानी _eol_ अनगिनत राही गए इस राह से _comma_ उनका पता क्या _eol_ पर गए कुछ लोग इस पर छोड़ पैरों की निशानी _eol_ यह निशानी मूक होकर भी बहुत कुछ बोलती है _eol_ खोल इसका अर्थ _comma_ पंथी _comma_ पंथ का अनुमान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। है अनिश्चित किस जगह पर सरित _comma_ गिरि _comma_ गह्वर मिलेंगे _eol_ है अनिश्चित किस जगह पर बाग वन सुंदर मिलेंगे _eol_ किस जगह यात्रा खतम हो जाएगी _comma_ यह भी अनिश्चित _eol_ है अनिश्चित कब सुमन _comma_ कब कंटकों के शर मिलेंगे कौन सहसा छूट जाएँगे _comma_ मिलेंगे कौन सहसा _eol_ आ पड़े कुछ भी _comma_ रुकेगा तू न _comma_ ऐसी आन कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। कौन कहता है कि स्वप्नों को न आने दे हृदय में _eol_ देखते सब हैं इन्हें अपनी उमर _comma_ अपने समय में _eol_ और तू कर यत्न भी तो _comma_ मिल नहीं सकती सफलता _eol_ ये उदय होते लिए कुछ ध्येय नयनों के निलय में _eol_ किंतु जग के पंथ पर यदि _comma_ स्वप्न दो तो सत्य दो सौ _eol_ स्वप्न पर ही मुग्ध मत हो _comma_ सत्य का भी ज्ञान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। स्वप्न आता स्वर्ग का _comma_ दृगकोरकों में दीप्ति आती _eol_ पंख लग जाते पगों को _comma_ ललकती उन्मुक्त छाती _eol_ रास्ते का एक काँटा _comma_ पाँव का दिल चीर देता _eol_ रक्त की दो बूँद गिरतीं _comma_ एक दुनिया डूब जाती _eol_ आँख में हो स्वर्ग लेकिन _comma_ पाँव पृथ्वी पर टिके हों _eol_ कंटकों की इस अनोखी सीख का सम्मान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। यह बुरा है या कि अच्छा _comma_ व्यर्थ दिन इस पर बिताना _eol_ अब असंभव छोड़ यह पथ दूसरे पर पग बढ़ाना _eol_ तू इसे अच्छा समझ _comma_ यात्रा सरल इससे बनेगी _eol_ सोच मत केवल तुझे ही यह पड़ा मन में बिठाना _eol_ हर सफल पंथी यही विश्वास ले इस पर बढ़ा है _eol_ तू इसी पर आज अपने चित्त का अवधान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। •• चल मरदाने •• चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । एक हमारा देश _comma_ हमारा वेश _comma_ हमारी कौम _comma_ हमारी मंज़िल _comma_ हम किससे भयभीत । चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । हम भारत की अमर जवानी _eol_ सागर की लहरें लासानी _eol_ गंगजमुन के निर्मल पानी _eol_ हिमगिरि की ऊंची पेशानी सबके प्रेरक _comma_ रक्षक _comma_ मीत । चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । जग के पथ पर जो न रुकेगा _eol_ जो न झुकेगा _comma_ जो न मुडेगा _eol_ उसका जीवन _comma_ उसकी जीत । चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । •• साथी _comma_ सब कुछ सहना होगा •• मानव पर जगती का शासन _eol_ जगती पर संसृति का बंधन _eol_ संसृति को भी और किसी के प्रतिबंधों में रहना होगा',\n",
       " ' साथी _comma_ सब कुछ सहना होगा',\n",
       " ' हम क्या हैं जगती के सर में',\n",
       " ' जगती क्या _comma_ संसृति सागर में',\n",
       " ' एक प्रबल धारा में हमको लघु तिनकेसा बहना होगा']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = text.split('.')\n",
    "training_data[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6b49497-0def-4931-969c-434adcd00fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poems by harivansh raI bachchan source<https _comma_//hindionlinejankari',\n",
       " 'com/harivanshraibachchanpoems/> & <https _comma_//hindikavita',\n",
       " 'com/hindipoetryharivanshraibachchan',\n",
       " 'php> •• अग्निपथ कविता •• वृक्ष हों भले खड़े _eol_ हों घने हों बड़े _eol_ एक पत्र छाँह भी _eol_ माँग मत _comma_ माँग मत _comma_ माँग मत _eol_ अग्निपथ अग्निपथ अग्निपथ। तू न थकेगा कभी _comma_ तू न रुकेगा कभी _eol_ तू न मुड़ेगा कभी _eol_ कर शपथ _comma_ कर शपथ _comma_ कर शपथ _eol_ अग्निपथ अग्निपथ अग्निपथ। यह महान दृश्य है _eol_ चल रहा मनुष्य है _eol_ अश्रु श्वेत रक्त से _eol_ लथपथ लथपथ लथपथ _eol_ अग्निपथ अग्निपथ अग्निपथ। ••• नीड़ का निर्माण ••• नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। वह उठी आँधी कि नभ में छा गया सहसा अँधेरा _eol_ धूलि धूसर बादलों ने भूमि को इस भाँति घेरा _eol_ रातसा दिन हो गया _comma_ फिर रात आ\\u200cई और काली _eol_ लग रहा था अब न होगा इस निशा का फिर सवेरा _eol_ रात के उत्पातभय से भीत जनजन _comma_ भीत कणकण किंतु प्राची से उषा की मोहिनी मुस्कान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। वह चले झोंके कि काँपे भीम कायावान भूधर _eol_ जड़ समेत उखड़पुखड़कर गिर पड़े _comma_ टूटे विटप वर _eol_ हाय _comma_ तिनकों से विनिर्मित घोंसलो पर क्या न बीती _eol_ डगमगा\\u200cए जबकि कंकड़ _eol_ ईंट _comma_ पत्थर के महलघर बोल आशा के विहंगम _eol_ किस जगह पर तू छिपा था _eol_ जो गगन पर चढ़ उठाता गर्व से निज तान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। क्रुद्ध नभ के वज्र दंतों में उषा है मुसकराती _eol_ घोर गर्जनमय गगन के कंठ में खग पंक्ति गाती _comma_ एक चिड़िया चोंच में तिनका लि\\u200cए जो जा रही है _eol_ वह सहज में ही पवन उंचास को नीचा दिखाती नाश के दुख से कभी दबता नहीं निर्माण का सुख प्रलय की निस्तब्धता से सृष्टि का नव गान फिरफिर नीड़ का निर्माण फिरफिर _eol_ नेह का आह्णान फिरफिर। •• पथ की पहचान •• पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले पुस्तकों में है नहीं छापी गई इसकी कहानी _eol_ हाल इसका ज्ञात होता है न औरों की जबानी _eol_ अनगिनत राही गए इस राह से _comma_ उनका पता क्या _eol_ पर गए कुछ लोग इस पर छोड़ पैरों की निशानी _eol_ यह निशानी मूक होकर भी बहुत कुछ बोलती है _eol_ खोल इसका अर्थ _comma_ पंथी _comma_ पंथ का अनुमान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। है अनिश्चित किस जगह पर सरित _comma_ गिरि _comma_ गह्वर मिलेंगे _eol_ है अनिश्चित किस जगह पर बाग वन सुंदर मिलेंगे _eol_ किस जगह यात्रा खतम हो जाएगी _comma_ यह भी अनिश्चित _eol_ है अनिश्चित कब सुमन _comma_ कब कंटकों के शर मिलेंगे कौन सहसा छूट जाएँगे _comma_ मिलेंगे कौन सहसा _eol_ आ पड़े कुछ भी _comma_ रुकेगा तू न _comma_ ऐसी आन कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। कौन कहता है कि स्वप्नों को न आने दे हृदय में _eol_ देखते सब हैं इन्हें अपनी उमर _comma_ अपने समय में _eol_ और तू कर यत्न भी तो _comma_ मिल नहीं सकती सफलता _eol_ ये उदय होते लिए कुछ ध्येय नयनों के निलय में _eol_ किंतु जग के पंथ पर यदि _comma_ स्वप्न दो तो सत्य दो सौ _eol_ स्वप्न पर ही मुग्ध मत हो _comma_ सत्य का भी ज्ञान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। स्वप्न आता स्वर्ग का _comma_ दृगकोरकों में दीप्ति आती _eol_ पंख लग जाते पगों को _comma_ ललकती उन्मुक्त छाती _eol_ रास्ते का एक काँटा _comma_ पाँव का दिल चीर देता _eol_ रक्त की दो बूँद गिरतीं _comma_ एक दुनिया डूब जाती _eol_ आँख में हो स्वर्ग लेकिन _comma_ पाँव पृथ्वी पर टिके हों _eol_ कंटकों की इस अनोखी सीख का सम्मान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। यह बुरा है या कि अच्छा _comma_ व्यर्थ दिन इस पर बिताना _eol_ अब असंभव छोड़ यह पथ दूसरे पर पग बढ़ाना _eol_ तू इसे अच्छा समझ _comma_ यात्रा सरल इससे बनेगी _eol_ सोच मत केवल तुझे ही यह पड़ा मन में बिठाना _eol_ हर सफल पंथी यही विश्वास ले इस पर बढ़ा है _eol_ तू इसी पर आज अपने चित्त का अवधान कर ले। पूर्व चलने के बटोही _comma_ बाट की पहचान कर ले। •• चल मरदाने •• चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । एक हमारा देश _comma_ हमारा वेश _comma_ हमारी कौम _comma_ हमारी मंज़िल _comma_ हम किससे भयभीत । चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । हम भारत की अमर जवानी _eol_ सागर की लहरें लासानी _eol_ गंगजमुन के निर्मल पानी _eol_ हिमगिरि की ऊंची पेशानी सबके प्रेरक _comma_ रक्षक _comma_ मीत । चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । जग के पथ पर जो न रुकेगा _eol_ जो न झुकेगा _comma_ जो न मुडेगा _eol_ उसका जीवन _comma_ उसकी जीत । चल मरदाने _comma_ सीना ताने _eol_ हाथ हिलाते _comma_ पांव बढाते _eol_ मन मुस्काते _comma_ गाते गीत । •• साथी _comma_ सब कुछ सहना होगा •• मानव पर जगती का शासन _eol_ जगती पर संसृति का बंधन _eol_ संसृति को भी और किसी के प्रतिबंधों में रहना होगा',\n",
       " 'साथी _comma_ सब कुछ सहना होगा',\n",
       " 'हम क्या हैं जगती के सर में',\n",
       " 'जगती क्या _comma_ संसृति सागर में',\n",
       " 'एक प्रबल धारा में हमको लघु तिनकेसा बहना होगा']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(training_data)):\n",
    "    training_data[i] = training_data[i].strip()\n",
    "training_data[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2cf99efa-818a-4037-98a5-1fa7f87a603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Set the maximum sequence length for padding\n",
    "max_sequence_length = 200\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(training_data)\n",
    " \n",
    "# Convert the training data into sequences of tokens\n",
    "sequences = tokenizer.texts_to_sequences(training_data)\n",
    " \n",
    "# Pad the sequences to have the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0bade5a5-e4a2-4d09-a7eb-23485a87f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[596, 597, 598, 599, 600, 601, 355, 2, 602]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cbfbef39-a30c-4710-af5c-6e80c24ec294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 596 597 598 599 600 601 355\n",
      "   2 602]\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "94ce831e-601d-4e7c-92d3-2ac0f3b3f299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa99ce3-21ae-44bf-a3e5-db00a62db7c9",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "324f3d87-62ec-40da-84f8-8e0b2c5db3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input and output data for training\n",
    "X = padded_sequences[:, :-1]\n",
    "y = padded_sequences[:, -1]\n",
    "\n",
    "# Get the total number of unique words in the training data\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4981f82-d292-4bf2-b204-1b2e0ee00343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 596 597 598 599 600 601 355\n",
      "   2 602] 200\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences[0], len(padded_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d795ddd1-21de-48b8-8702-0ca54746f2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0 596 597 598 599 600 601 355\n",
      "   2] 199\n"
     ]
    }
   ],
   "source": [
    "print(X[0], len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a41103e3-f020-4561-a80b-f72ac2ee6a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef65a6-cbef-45c7-8d8a-255e192dd6e1",
   "metadata": {},
   "source": [
    "## Complete training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2e46cddb-3edd-4a69-ab14-74b44ab063c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[596, 597, 598, 599, 600, 601, 355, 2, 602]\n",
      "[596] 597\n",
      "[596, 597] 598\n",
      "[596, 597, 598] 599\n",
      "[596, 597, 598, 599] 600\n",
      "[596, 597, 598, 599, 600] 601\n",
      "[596, 597, 598, 599, 600, 601] 355\n",
      "[596, 597, 598, 599, 600, 601, 355] 2\n",
      "[596, 597, 598, 599, 600, 601, 355, 2] 602\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "#for s in sequences:\n",
    "print(sequences[0])\n",
    "for s in [sequences[0]]:\n",
    "    #seq_array = s.split()\n",
    "    words_in_s = [s[0]]\n",
    "    for w in s[1:]:\n",
    "        X.append([v for v in words_in_s])\n",
    "        y.append(w)\n",
    "        words_in_s.append(w)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d750040-a567-47dd-a686-a3dcdbbe8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 9788.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "junk = [str(i) for i in range(10)]\n",
    "from tqdm import tqdm\n",
    "for s in tqdm(junk):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ebfc6363-fff9-42a3-bb9b-898a510cc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 1964.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5390, 5390)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for s in tqdm(sequences):\n",
    "    if 0 < len(s):\n",
    "        words_in_s = [s[0]]\n",
    "        for w in s[1:]:\n",
    "            X.append([v for v in words_in_s])\n",
    "            y.append(w)\n",
    "            words_in_s.append(w)\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9a8b5ff3-6de6-470c-b3e7-2bd036432ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[606, 10, 110, 218, 10, 607, 134, 608, 609, 1, 134, 610, 134, 611, 1, 22, 612, 357, 24, 1, 219, 39, 2, 219, 39, 2, 219, 39, 1, 110, 110, 275, 27, 13, 613, 72, 2, 27, 13, 276, 72, 1, 27, 13, 614, 72, 1, 12, 277, 2, 12, 277, 2, 12, 277, 1, 110, 110, 275, 19, 358, 615, 3, 1, 73, 97, 616, 3, 1, 359, 617, 360, 7, 1, 278, 278, 278, 1, 110, 110, 275, 361, 161, 5, 135, 361, 161, 5] 135\n"
     ]
    }
   ],
   "source": [
    "print(X[100], y[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b4fd64b-91ab-4724-8ad4-8412bd69fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[606, 10, 110, 218, 10, 607, 134, 608, 609, 1, 134, 610, 134, 611, 1, 22, 612, 357, 24, 1, 219, 39, 2, 219, 39, 2, 219, 39, 1, 110, 110, 275, 27, 13, 613, 72, 2, 27, 13, 276, 72, 1, 27, 13, 614, 72, 1, 12, 277, 2, 12, 277, 2, 12, 277, 1, 110, 110, 275, 19, 358, 615, 3, 1, 73, 97, 616, 3, 1, 359, 617, 360, 7, 1, 278, 278, 278, 1, 110, 110, 275, 361, 161, 5, 135, 361, 161, 5, 135] 111\n"
     ]
    }
   ],
   "source": [
    "print(X[101], y[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ecdaadc4-a51d-4481-842c-9d764c022a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 596])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "padded_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d177978d-10e1-4222-855d-c1997c62916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5390,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = np.array(y)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d4cc0-e259-4309-a062-ce270bb80343",
   "metadata": {},
   "source": [
    "## Load embedding for both Keras Enbedding and RNN Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590176d-96aa-4210-80a6-8ab4f1e609b9",
   "metadata": {},
   "source": [
    "## 1.Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32398762-7514-4381-937f-5d64f8d1e132",
   "metadata": {},
   "source": [
    "### 2.1 Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8405722-48eb-4487-9793-2c81080eef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1601 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "glove_dir = \"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/\"\n",
    "\n",
    "embeddings_index = {} #initialize dictionary\n",
    "f = open(os.path.join(glove_dir, 'D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_keras_embeddings5.txt'), encoding='utf8')\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "except:\n",
    "    print(line)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc3bf603-43c7-44b9-96de-f1163906f237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f5285c8-a56f-40cb-a5e4-2dfa468775e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary_size = vocab_size\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2f776bd-f786-4fea-b80b-56ccf499940d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1630, 100)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f300169a-33a0-410b-8ddf-bbd443f33ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.04580086,  0.01601617,  0.02020026, ...,  0.00669035,\n",
       "         0.04210837,  0.0457218 ],\n",
       "       [-0.02822907,  0.0343956 , -0.04777978, ...,  0.02365724,\n",
       "         0.02021687, -0.04505454],\n",
       "       [ 0.00803049, -0.04102447,  0.00462417, ...,  0.02595557,\n",
       "        -0.00771595, -0.00576849]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d1fcb-ed81-4296-8f83-4129e5f591f9",
   "metadata": {},
   "source": [
    "### 2.2 Build model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c9f52565-f2f4-4ec8-853d-037925dcbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d3e6ef45-9041-4efb-a419-e8d073dd3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your parameters (ensure these are correctly initialized)\n",
    "num_tokens = 1630  # Example value for the vocabulary size\n",
    "embedding_dim = 100  # Example dimension for the embedding\n",
    "embedding_matrix = tf.random.normal((num_tokens, embedding_dim))  # Example random embedding matrix\n",
    "vocab_size = 1630  # The number of unique tokens in your vocabulary\n",
    "max_sequence_length = 100  # The input sequence length\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_tokens,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    input_length=max_sequence_length,\n",
    "    trainable=False\n",
    "))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8c97dcd2-20e6-4710-b8ff-7a429ed1bf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1630</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,630</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m163,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_21 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m80,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1630\u001b[0m)                │         \u001b[38;5;34m164,630\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">408,030</span> (1.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m408,030\u001b[0m (1.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">245,030</span> (957.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m245,030\u001b[0m (957.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">163,000</span> (636.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m163,000\u001b[0m (636.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the model summary\n",
    "model.build(input_shape=(None, max_sequence_length))  # Explicitly define the input shape\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a14c1-25b8-44e0-8733-3676ae256301",
   "metadata": {},
   "source": [
    "### 2.3 Train the model for Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7b7b0b7e-8d61-4bb3-9ab8-30ec7ceb86b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 109ms/step - accuracy: 0.0373 - loss: 7.0097\n",
      "CPU times: total: 1min 7s\n",
      "Wall time: 21.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29e58081ba0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(padded_X, y2, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1029e2ad-f002-48b3-8aa5-0dba0e1e9226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 109ms/step - accuracy: 0.0738 - loss: 6.0947\n",
      "Epoch 2/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 111ms/step - accuracy: 0.0787 - loss: 5.8254\n",
      "Epoch 3/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.0936 - loss: 5.4349\n",
      "Epoch 4/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.1258 - loss: 5.0081\n",
      "Epoch 5/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.1843 - loss: 4.6115\n",
      "Epoch 6/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.2271 - loss: 4.2153\n",
      "Epoch 7/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.2690 - loss: 3.8271\n",
      "Epoch 8/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.3196 - loss: 3.4358\n",
      "Epoch 9/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.3667 - loss: 3.1162\n",
      "Epoch 10/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.4340 - loss: 2.7878\n",
      "Epoch 11/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.5001 - loss: 2.5252\n",
      "Epoch 12/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.5572 - loss: 2.2970\n",
      "Epoch 13/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.6271 - loss: 2.0444\n",
      "Epoch 14/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.6786 - loss: 1.8511\n",
      "Epoch 15/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.7399 - loss: 1.6114\n",
      "Epoch 16/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.7589 - loss: 1.4448\n",
      "Epoch 17/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.8078 - loss: 1.2974\n",
      "Epoch 18/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.8343 - loss: 1.1606\n",
      "Epoch 19/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.8622 - loss: 1.0331\n",
      "Epoch 20/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.8797 - loss: 0.9397\n",
      "Epoch 21/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.8912 - loss: 0.8496\n",
      "Epoch 22/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9152 - loss: 0.7609\n",
      "Epoch 23/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9320 - loss: 0.6650\n",
      "Epoch 24/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9410 - loss: 0.5951\n",
      "Epoch 25/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9483 - loss: 0.5685\n",
      "Epoch 26/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9551 - loss: 0.4915\n",
      "Epoch 27/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 63ms/step - accuracy: 0.9612 - loss: 0.4346\n",
      "Epoch 28/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.9720 - loss: 0.4001\n",
      "Epoch 29/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9740 - loss: 0.3542\n",
      "Epoch 30/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9778 - loss: 0.3208\n",
      "Epoch 31/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9800 - loss: 0.2910\n",
      "Epoch 32/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9827 - loss: 0.2675\n",
      "Epoch 33/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9844 - loss: 0.2353\n",
      "Epoch 34/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9838 - loss: 0.2250\n",
      "Epoch 35/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9894 - loss: 0.1929\n",
      "Epoch 36/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.9877 - loss: 0.1788\n",
      "Epoch 37/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9904 - loss: 0.1601\n",
      "Epoch 38/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9916 - loss: 0.1394\n",
      "Epoch 39/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9919 - loss: 0.1278\n",
      "Epoch 40/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9882 - loss: 0.1285\n",
      "Epoch 41/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9914 - loss: 0.1149\n",
      "Epoch 42/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9908 - loss: 0.1083\n",
      "Epoch 43/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9922 - loss: 0.0971\n",
      "Epoch 44/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9922 - loss: 0.0890\n",
      "Epoch 45/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9915 - loss: 0.0820\n",
      "Epoch 46/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9913 - loss: 0.0789\n",
      "Epoch 47/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 64ms/step - accuracy: 0.9921 - loss: 0.0726\n",
      "Epoch 48/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9900 - loss: 0.0710\n",
      "Epoch 49/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9932 - loss: 0.0622\n",
      "Epoch 50/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9922 - loss: 0.0589\n",
      "Epoch 51/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9915 - loss: 0.0559\n",
      "Epoch 52/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9922 - loss: 0.0581\n",
      "Epoch 53/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9928 - loss: 0.0500\n",
      "Epoch 54/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9938 - loss: 0.0457\n",
      "Epoch 55/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9937 - loss: 0.0436\n",
      "Epoch 56/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 65ms/step - accuracy: 0.9930 - loss: 0.0388\n",
      "Epoch 57/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9922 - loss: 0.0451\n",
      "Epoch 58/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9925 - loss: 0.0379\n",
      "Epoch 59/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9947 - loss: 0.0336\n",
      "Epoch 60/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9914 - loss: 0.0398\n",
      "Epoch 61/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9941 - loss: 0.0312\n",
      "Epoch 62/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9909 - loss: 0.0367\n",
      "Epoch 63/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9677 - loss: 0.1697\n",
      "Epoch 64/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9864 - loss: 0.0803\n",
      "Epoch 65/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9943 - loss: 0.0392\n",
      "Epoch 66/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9920 - loss: 0.0371\n",
      "Epoch 67/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9934 - loss: 0.0322\n",
      "Epoch 68/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9905 - loss: 0.0329\n",
      "Epoch 69/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9931 - loss: 0.0299\n",
      "Epoch 70/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9927 - loss: 0.0285\n",
      "Epoch 71/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9950 - loss: 0.0225\n",
      "Epoch 72/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9939 - loss: 0.0232\n",
      "Epoch 73/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.9926 - loss: 0.0264\n",
      "Epoch 74/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0215\n",
      "Epoch 75/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9925 - loss: 0.0249\n",
      "Epoch 76/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9929 - loss: 0.0247\n",
      "Epoch 77/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9934 - loss: 0.0233\n",
      "Epoch 78/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9932 - loss: 0.0205\n",
      "Epoch 79/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9922 - loss: 0.0225\n",
      "Epoch 80/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9910 - loss: 0.0242\n",
      "Epoch 81/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9932 - loss: 0.0223\n",
      "Epoch 82/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9932 - loss: 0.0200\n",
      "Epoch 83/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9932 - loss: 0.0207\n",
      "Epoch 84/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9955 - loss: 0.0178\n",
      "Epoch 85/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9921 - loss: 0.0240\n",
      "Epoch 86/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9821 - loss: 0.0852\n",
      "Epoch 87/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.9820 - loss: 0.0800\n",
      "Epoch 88/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9930 - loss: 0.0331\n",
      "Epoch 89/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.9937 - loss: 0.0239\n",
      "Epoch 90/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.9940 - loss: 0.0213\n",
      "Epoch 91/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9941 - loss: 0.0212\n",
      "Epoch 92/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9928 - loss: 0.0218\n",
      "Epoch 93/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.9943 - loss: 0.0170\n",
      "Epoch 94/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9935 - loss: 0.0202\n",
      "Epoch 95/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9932 - loss: 0.0190\n",
      "Epoch 96/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9896 - loss: 0.0246\n",
      "Epoch 97/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9930 - loss: 0.0216\n",
      "Epoch 98/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9934 - loss: 0.0191\n",
      "Epoch 99/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9928 - loss: 0.0199\n",
      "Epoch 100/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9931 - loss: 0.0175\n",
      "Epoch 101/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9934 - loss: 0.0189\n",
      "Epoch 102/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9937 - loss: 0.0178\n",
      "Epoch 103/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9942 - loss: 0.0155\n",
      "Epoch 104/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.9931 - loss: 0.0165\n",
      "Epoch 105/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9948 - loss: 0.0167\n",
      "Epoch 106/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9952 - loss: 0.0164\n",
      "Epoch 107/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9936 - loss: 0.0164\n",
      "Epoch 108/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9935 - loss: 0.0171\n",
      "Epoch 109/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9932 - loss: 0.0183\n",
      "Epoch 110/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9933 - loss: 0.0180\n",
      "Epoch 111/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9935 - loss: 0.0176\n",
      "Epoch 112/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9950 - loss: 0.0137\n",
      "Epoch 113/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9941 - loss: 0.0177\n",
      "Epoch 114/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9923 - loss: 0.0194\n",
      "Epoch 115/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9927 - loss: 0.0168\n",
      "Epoch 116/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9937 - loss: 0.0172\n",
      "Epoch 117/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9930 - loss: 0.0192\n",
      "Epoch 118/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9944 - loss: 0.0130\n",
      "Epoch 119/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9947 - loss: 0.0133\n",
      "Epoch 120/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9943 - loss: 0.0146\n",
      "Epoch 121/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9927 - loss: 0.0188\n",
      "Epoch 122/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9792 - loss: 0.0707\n",
      "Epoch 123/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9699 - loss: 0.1134\n",
      "Epoch 124/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9906 - loss: 0.0371\n",
      "Epoch 125/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9938 - loss: 0.0215\n",
      "Epoch 126/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9931 - loss: 0.0188\n",
      "Epoch 127/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - accuracy: 0.9912 - loss: 0.0257\n",
      "Epoch 128/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9931 - loss: 0.0210\n",
      "Epoch 129/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.9938 - loss: 0.0175\n",
      "Epoch 130/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9935 - loss: 0.0179\n",
      "Epoch 131/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9927 - loss: 0.0203\n",
      "Epoch 132/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9926 - loss: 0.0193\n",
      "Epoch 133/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9934 - loss: 0.0161\n",
      "Epoch 134/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.9940 - loss: 0.0162\n",
      "Epoch 135/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9951 - loss: 0.0129\n",
      "Epoch 136/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9947 - loss: 0.0151\n",
      "Epoch 137/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9941 - loss: 0.0164\n",
      "Epoch 138/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9921 - loss: 0.0174\n",
      "Epoch 139/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.9955 - loss: 0.0117\n",
      "Epoch 140/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9941 - loss: 0.0158\n",
      "Epoch 141/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.9911 - loss: 0.0190\n",
      "Epoch 142/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9938 - loss: 0.0161\n",
      "Epoch 143/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9939 - loss: 0.0149\n",
      "Epoch 144/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9937 - loss: 0.0157\n",
      "Epoch 145/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 67ms/step - accuracy: 0.9923 - loss: 0.0200\n",
      "Epoch 146/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 68ms/step - accuracy: 0.9925 - loss: 0.0180\n",
      "Epoch 147/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.9918 - loss: 0.0185\n",
      "Epoch 148/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9940 - loss: 0.0156\n",
      "Epoch 149/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.9956 - loss: 0.0114\n",
      "Epoch 150/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9929 - loss: 0.0166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29e2c69eb60>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_X, y2, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eec40048-ffb4-4224-a7ba-1c721e6dce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved successfully!\n",
      "Model weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "# Save only the model architecture\n",
    "model_json = model.to_json()\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_architecture_keras.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Model architecture saved successfully!\")\n",
    " \n",
    "# Save the model weights\n",
    "model.save_weights('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_weights_keras.weights.h5')\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c4dda43c-0cfa-4bd0-b0ea-722dd866b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture loaded successfully!\n",
      "Weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "# Load the model architecture\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_architecture_keras.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json)\n",
    "print(\"Model architecture loaded successfully!\")\n",
    "\n",
    "# Assuming 'model' is your already defined model with the correct architecture\n",
    "model.load_weights('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_weights_keras.weights.h5')\n",
    " \n",
    "print(\"Weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c14ef-b9f4-4ce9-be89-c8b8303b644d",
   "metadata": {},
   "source": [
    "## Generate Text for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "83aeee27-9d4d-4d2a-811f-17eceee97a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 5 sentences for the prompt: 'वृक्ष हों भले खड़े'\n",
      "\n",
      "Generated sentence 1: वृक्ष हों भले खड़े eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत eol\n",
      "\n",
      "Generated sentence 2: वृक्ष हों भले खड़े eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत eol\n",
      "\n",
      "Generated sentence 3: वृक्ष हों भले खड़े eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत eol\n",
      "\n",
      "Generated sentence 4: वृक्ष हों भले खड़े eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत eol\n",
      "\n",
      "Generated sentence 5: वृक्ष हों भले खड़े eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत eol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(model, tokenizer, prompt, max_sequence_length, next_words=20):\n",
    "    generated_text = prompt\n",
    "    for _ in range(next_words):\n",
    "        # Tokenize and pad the current text\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    " \n",
    "        # Predict the next word\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted, axis=-1)[0]\n",
    " \n",
    "        # Convert the predicted index to word\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    " \n",
    "        # Add the predicted word to the generated text\n",
    "        generated_text += \" \" + output_word\n",
    " \n",
    "        # Stop if a period (end of sentence) is generated\n",
    "        if output_word == '.':\n",
    "            break\n",
    " \n",
    "    # Replace 'comma' with ',' in the generated sentence\n",
    "    generated_text = generated_text.replace(' comma ', ', ')\n",
    " \n",
    "    return generated_text\n",
    " \n",
    "# Set the maximum sequence length (should match your model's input configuration)\n",
    "max_sequence_length = 100  # Adjust to match your model's configuration\n",
    " \n",
    "# Define the prompt just once\n",
    "prompt = \"वृक्ष हों भले खड़े\" # कर शपथ, कर शपथ, कर शपथ,\n",
    " \n",
    "# Generate at least 5 different sentences for the prompt\n",
    "print(f\"\\nGenerating 5 sentences for the prompt: '{prompt}'\\n\")\n",
    "for i in range(5):\n",
    "    generated_sentence = generate_text(model, tokenizer, prompt, max_sequence_length)\n",
    "    print(f\"Generated sentence {i + 1}: {generated_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06666502-5230-42ef-b4c4-416760a90607",
   "metadata": {},
   "source": [
    "## 2.RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900287d-c5ee-43d8-9ef0-13e1736f868a",
   "metadata": {},
   "source": [
    "### 2.1 Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "47c18233-0b54-4d2b-ad29-227796bb374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1562 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "glove_dir = \"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/\"\n",
    "\n",
    "embeddings_index = {}  # initialize dictionary\n",
    "with open(os.path.join(glove_dir, 'hindi_rnn_embeddings5.txt'), encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)  # Split only at the first space\n",
    "        if len(parts) == 2:\n",
    "            word, vector_str = parts\n",
    "            try:\n",
    "                vector = np.fromstring(vector_str, sep=' ', dtype='float32')\n",
    "                embeddings_index[word] = vector\n",
    "            except ValueError:\n",
    "                print(f\"Error processing line: {line[:50]}...\")  # Print first 50 chars of problematic line\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cbae1a52-b75f-4b97-8e3a-824b95c74af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9494c6d5-2a86-45ce-92b3-bfec3f6e7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "vocabulary_size = vocab_size\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d5e72bdb-180b-4fc7-a4d9-1ef4451c5cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1630, 256)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "630fe376-02e1-412a-b8d4-5e67613f0884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.29197177, 0.14049494, 0.89266354, ..., 0.09321079, 0.074166  ,\n",
       "        0.14163157],\n",
       "       [0.82780212, 0.64313257, 0.2325514 , ..., 0.95406705, 0.89009041,\n",
       "        0.8139115 ],\n",
       "       [0.21626054, 0.00721073, 0.74732375, ..., 0.30306888, 0.0416267 ,\n",
       "        0.1544755 ]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428ebc2-688c-4254-a0bf-1db6da92e4d3",
   "metadata": {},
   "source": [
    "### 2.2 Build model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ce7a03f6-4b96-43c3-a73c-5bffb3ec6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b204e42c-bb0c-4baa-9e7f-d6a5d9c709e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define your parameters (ensure these are correctly initialized)\n",
    "num_tokens = 1630  # Example value for the vocabulary size\n",
    "embedding_dim = 256  # Example dimension for the embedding\n",
    "embedding_matrix = tf.random.normal((num_tokens, embedding_dim))  # Example random embedding matrix\n",
    "vocab_size = 1630  # The number of unique tokens in your vocabulary\n",
    "max_sequence_length = 100  # The input sequence length\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=num_tokens,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    input_length=max_sequence_length,\n",
    "    trainable=False\n",
    "))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b0edd867-7d2d-48a7-ad17-ac1c735ed4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">417,280</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">142,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1630</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,630</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m417,280\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_23 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │         \u001b[38;5;34m142,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1630\u001b[0m)                │         \u001b[38;5;34m164,630\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">724,710</span> (2.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m724,710\u001b[0m (2.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">307,430</span> (1.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m307,430\u001b[0m (1.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">417,280</span> (1.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m417,280\u001b[0m (1.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the model summary\n",
    "model.build(input_shape=(None, max_sequence_length))  # Explicitly define the input shape\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d45abd-2e37-495c-805d-8ba43452b6eb",
   "metadata": {},
   "source": [
    "### 2.3 Train the model for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c1e82ede-446f-446d-9eba-39a573be3b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.0446 - loss: 6.9850\n",
      "CPU times: total: 58.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29e59aa7610>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(padded_X, y2, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d385c1be-3c1d-4f00-83e2-5907b62a73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.0753 - loss: 5.9937\n",
      "Epoch 2/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.1061 - loss: 5.4948\n",
      "Epoch 3/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.1505 - loss: 4.9998\n",
      "Epoch 4/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.2213 - loss: 4.4266\n",
      "Epoch 5/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.2846 - loss: 3.9935\n",
      "Epoch 6/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.3454 - loss: 3.4895\n",
      "Epoch 7/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.3980 - loss: 3.0700\n",
      "Epoch 8/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.4518 - loss: 2.7653\n",
      "Epoch 9/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.5297 - loss: 2.4042\n",
      "Epoch 10/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.6217 - loss: 2.0866\n",
      "Epoch 11/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.7016 - loss: 1.7794\n",
      "Epoch 12/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 71ms/step - accuracy: 0.7649 - loss: 1.5665\n",
      "Epoch 13/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.8239 - loss: 1.3472\n",
      "Epoch 14/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.8579 - loss: 1.1567\n",
      "Epoch 15/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.8971 - loss: 0.9721\n",
      "Epoch 16/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.9143 - loss: 0.8493\n",
      "Epoch 17/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9330 - loss: 0.7353\n",
      "Epoch 18/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9438 - loss: 0.6275\n",
      "Epoch 19/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9565 - loss: 0.5460\n",
      "Epoch 20/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.9666 - loss: 0.4790\n",
      "Epoch 21/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.9720 - loss: 0.4058\n",
      "Epoch 22/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9789 - loss: 0.3537\n",
      "Epoch 23/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9832 - loss: 0.3046\n",
      "Epoch 24/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 80ms/step - accuracy: 0.9828 - loss: 0.2711\n",
      "Epoch 25/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9829 - loss: 0.2387\n",
      "Epoch 26/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9839 - loss: 0.2192\n",
      "Epoch 27/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9835 - loss: 0.1971\n",
      "Epoch 28/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9887 - loss: 0.1662\n",
      "Epoch 29/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.9854 - loss: 0.1616\n",
      "Epoch 30/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9873 - loss: 0.1410\n",
      "Epoch 31/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9858 - loss: 0.1340\n",
      "Epoch 32/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.9883 - loss: 0.1156\n",
      "Epoch 33/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9924 - loss: 0.1011\n",
      "Epoch 34/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.9898 - loss: 0.0931\n",
      "Epoch 35/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.9893 - loss: 0.0921\n",
      "Epoch 36/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.9901 - loss: 0.0838\n",
      "Epoch 37/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9883 - loss: 0.0805\n",
      "Epoch 38/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.9888 - loss: 0.0751\n",
      "Epoch 39/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9922 - loss: 0.0650\n",
      "Epoch 40/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9928 - loss: 0.0605\n",
      "Epoch 41/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9922 - loss: 0.0538\n",
      "Epoch 42/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9900 - loss: 0.0556\n",
      "Epoch 43/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 80ms/step - accuracy: 0.9936 - loss: 0.0481\n",
      "Epoch 44/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9930 - loss: 0.0453\n",
      "Epoch 45/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - accuracy: 0.9911 - loss: 0.0483\n",
      "Epoch 46/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.9924 - loss: 0.0421\n",
      "Epoch 47/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9951 - loss: 0.0376\n",
      "Epoch 48/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9899 - loss: 0.0465\n",
      "Epoch 49/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 79ms/step - accuracy: 0.9931 - loss: 0.0363\n",
      "Epoch 50/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9911 - loss: 0.0397\n",
      "Epoch 51/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9921 - loss: 0.0374\n",
      "Epoch 52/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9932 - loss: 0.0294\n",
      "Epoch 53/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9918 - loss: 0.0324\n",
      "Epoch 54/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.9910 - loss: 0.0311\n",
      "Epoch 55/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.9914 - loss: 0.0299\n",
      "Epoch 56/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 97ms/step - accuracy: 0.9932 - loss: 0.0283\n",
      "Epoch 57/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 80ms/step - accuracy: 0.9882 - loss: 0.0365\n",
      "Epoch 58/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9873 - loss: 0.0650\n",
      "Epoch 59/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9873 - loss: 0.0758\n",
      "Epoch 60/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 74ms/step - accuracy: 0.9911 - loss: 0.0467\n",
      "Epoch 61/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9917 - loss: 0.0329\n",
      "Epoch 62/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9922 - loss: 0.0275\n",
      "Epoch 63/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9934 - loss: 0.0246\n",
      "Epoch 64/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9935 - loss: 0.0238\n",
      "Epoch 65/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9941 - loss: 0.0246\n",
      "Epoch 66/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.9937 - loss: 0.0203\n",
      "Epoch 67/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - accuracy: 0.9930 - loss: 0.0224\n",
      "Epoch 68/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - accuracy: 0.9915 - loss: 0.0275\n",
      "Epoch 69/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9939 - loss: 0.0191\n",
      "Epoch 70/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9917 - loss: 0.0244\n",
      "Epoch 71/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9921 - loss: 0.0201\n",
      "Epoch 72/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.9938 - loss: 0.0197\n",
      "Epoch 73/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9938 - loss: 0.0191\n",
      "Epoch 74/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9935 - loss: 0.0193\n",
      "Epoch 75/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9944 - loss: 0.0162\n",
      "Epoch 76/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 99ms/step - accuracy: 0.9936 - loss: 0.0179\n",
      "Epoch 77/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9908 - loss: 0.0234\n",
      "Epoch 78/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 99ms/step - accuracy: 0.9943 - loss: 0.0164\n",
      "Epoch 79/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9919 - loss: 0.0229\n",
      "Epoch 80/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 97ms/step - accuracy: 0.9897 - loss: 0.0345\n",
      "Epoch 81/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9783 - loss: 0.0873\n",
      "Epoch 82/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9883 - loss: 0.0434\n",
      "Epoch 83/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 102ms/step - accuracy: 0.9932 - loss: 0.0240\n",
      "Epoch 84/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 91ms/step - accuracy: 0.9916 - loss: 0.0232\n",
      "Epoch 85/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9936 - loss: 0.0217\n",
      "Epoch 86/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9949 - loss: 0.0166\n",
      "Epoch 87/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9946 - loss: 0.0175\n",
      "Epoch 88/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.9950 - loss: 0.0135\n",
      "Epoch 89/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 97ms/step - accuracy: 0.9914 - loss: 0.0209\n",
      "Epoch 90/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9923 - loss: 0.0212\n",
      "Epoch 91/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9940 - loss: 0.0173\n",
      "Epoch 92/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9938 - loss: 0.0152\n",
      "Epoch 93/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 100ms/step - accuracy: 0.9906 - loss: 0.0226\n",
      "Epoch 94/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 98ms/step - accuracy: 0.9925 - loss: 0.0199\n",
      "Epoch 95/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9911 - loss: 0.0203\n",
      "Epoch 96/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9954 - loss: 0.0126\n",
      "Epoch 97/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9932 - loss: 0.0176\n",
      "Epoch 98/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9930 - loss: 0.0173\n",
      "Epoch 99/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9932 - loss: 0.0167\n",
      "Epoch 100/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9946 - loss: 0.0142\n",
      "Epoch 101/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9915 - loss: 0.0196\n",
      "Epoch 102/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 85ms/step - accuracy: 0.9928 - loss: 0.0162\n",
      "Epoch 103/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9935 - loss: 0.0160\n",
      "Epoch 104/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9930 - loss: 0.0166\n",
      "Epoch 105/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9922 - loss: 0.0196\n",
      "Epoch 106/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 97ms/step - accuracy: 0.9930 - loss: 0.0168\n",
      "Epoch 107/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9940 - loss: 0.0145\n",
      "Epoch 108/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 100ms/step - accuracy: 0.9935 - loss: 0.0161\n",
      "Epoch 109/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9917 - loss: 0.0192\n",
      "Epoch 110/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9925 - loss: 0.0240\n",
      "Epoch 111/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9729 - loss: 0.1029\n",
      "Epoch 112/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9903 - loss: 0.0437\n",
      "Epoch 113/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9935 - loss: 0.0231\n",
      "Epoch 114/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9937 - loss: 0.0189\n",
      "Epoch 115/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 106ms/step - accuracy: 0.9940 - loss: 0.0178\n",
      "Epoch 116/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9916 - loss: 0.0205\n",
      "Epoch 117/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9934 - loss: 0.0172\n",
      "Epoch 118/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9954 - loss: 0.0159\n",
      "Epoch 119/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 99ms/step - accuracy: 0.9928 - loss: 0.0192\n",
      "Epoch 120/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9935 - loss: 0.0172\n",
      "Epoch 121/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9943 - loss: 0.0154\n",
      "Epoch 122/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 83ms/step - accuracy: 0.9946 - loss: 0.0140\n",
      "Epoch 123/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 101ms/step - accuracy: 0.9931 - loss: 0.0163\n",
      "Epoch 124/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9925 - loss: 0.0190\n",
      "Epoch 125/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 91ms/step - accuracy: 0.9937 - loss: 0.0155\n",
      "Epoch 126/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9935 - loss: 0.0160\n",
      "Epoch 127/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 92ms/step - accuracy: 0.9944 - loss: 0.0143\n",
      "Epoch 128/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9923 - loss: 0.0195\n",
      "Epoch 129/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 78ms/step - accuracy: 0.9912 - loss: 0.0193\n",
      "Epoch 130/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9941 - loss: 0.0146\n",
      "Epoch 131/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 77ms/step - accuracy: 0.9905 - loss: 0.0216\n",
      "Epoch 132/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - accuracy: 0.9936 - loss: 0.0168\n",
      "Epoch 133/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 94ms/step - accuracy: 0.9928 - loss: 0.0191\n",
      "Epoch 134/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 97ms/step - accuracy: 0.9912 - loss: 0.0200\n",
      "Epoch 135/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9925 - loss: 0.0161\n",
      "Epoch 136/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 110ms/step - accuracy: 0.9948 - loss: 0.0147\n",
      "Epoch 137/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9924 - loss: 0.0171\n",
      "Epoch 138/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9936 - loss: 0.0163\n",
      "Epoch 139/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - accuracy: 0.9940 - loss: 0.0145\n",
      "Epoch 140/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 101ms/step - accuracy: 0.9940 - loss: 0.0136\n",
      "Epoch 141/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9953 - loss: 0.0126\n",
      "Epoch 142/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9913 - loss: 0.0179\n",
      "Epoch 143/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 81ms/step - accuracy: 0.9930 - loss: 0.0163\n",
      "Epoch 144/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 89ms/step - accuracy: 0.9932 - loss: 0.0170\n",
      "Epoch 145/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 75ms/step - accuracy: 0.9937 - loss: 0.0169\n",
      "Epoch 146/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 90ms/step - accuracy: 0.9946 - loss: 0.0131\n",
      "Epoch 147/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 76ms/step - accuracy: 0.9941 - loss: 0.0133\n",
      "Epoch 148/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 95ms/step - accuracy: 0.9934 - loss: 0.0146\n",
      "Epoch 149/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - accuracy: 0.9943 - loss: 0.0147\n",
      "Epoch 150/150\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 93ms/step - accuracy: 0.9942 - loss: 0.0135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29e811a2f80>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_X, y2, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1a77db02-b6f2-46ca-bf11-dd5f014821b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture saved successfully!\n",
      "Model weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "# Save only the model architecture\n",
    "model_json = model.to_json()\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_architecture_RNN.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Model architecture saved successfully!\")\n",
    " \n",
    "# Save the model weights\n",
    "model.save_weights('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_weights_RNN.weights.h5')\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8b5deaab-2525-43d0-b4c6-b24520b90208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture loaded successfully!\n",
      "Weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "# Load the model architecture\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_architecture_RNN.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "model = model_from_json(loaded_model_json)\n",
    "print(\"Model architecture loaded successfully!\")\n",
    "\n",
    "# Assuming 'model' is your already defined model with the correct architecture\n",
    "model.load_weights('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_weights_RNN.weights.h5')\n",
    " \n",
    "print(\"Weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578fa76-8a9c-44b7-8c1c-249845e2e8a0",
   "metadata": {},
   "source": [
    "## Generate Text for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "80edb99c-de82-4bc9-8062-28fc4d57779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 5 sentences for the prompt: 'वृक्ष हों भले खड़े'\n",
      "\n",
      "Generated sentence 1: वृक्ष हों भले खड़े था eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत\n",
      "\n",
      "Generated sentence 2: वृक्ष हों भले खड़े था eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत\n",
      "\n",
      "Generated sentence 3: वृक्ष हों भले खड़े था eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत\n",
      "\n",
      "Generated sentence 4: वृक्ष हों भले खड़े था eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत\n",
      "\n",
      "Generated sentence 5: वृक्ष हों भले खड़े था eol हों घने हों बड़े eol एक पत्र छाँह भी eol माँग मत, माँग मत, माँग मत\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# Function to generate text based on a prompt\n",
    "def generate_text(model, tokenizer, prompt, max_sequence_length, next_words=20):\n",
    "    generated_text = prompt\n",
    "    for _ in range(next_words):\n",
    "        # Tokenize and pad the current text\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    " \n",
    "        # Predict the next word\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted, axis=-1)[0]\n",
    " \n",
    "        # Convert the predicted index to word\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    " \n",
    "        # Add the predicted word to the generated text\n",
    "        generated_text += \" \" + output_word\n",
    " \n",
    "        # Stop if a period (end of sentence) is generated\n",
    "        if output_word == '.':\n",
    "            break\n",
    " \n",
    "    # Replace 'comma' with ',' in the generated sentence\n",
    "    generated_text = generated_text.replace(' comma ', ', ')\n",
    " \n",
    "    return generated_text\n",
    " \n",
    "# Set the maximum sequence length (should match your model's input configuration)\n",
    "max_sequence_length = 100  # Adjust to match your model's configuration\n",
    " \n",
    "# Define the prompt just once\n",
    "prompt = \"वृक्ष हों भले खड़े\" # कर शपथ, कर शपथ, कर शपथ,\n",
    " \n",
    "# Generate at least 5 different sentences for the prompt\n",
    "print(f\"\\nGenerating 5 sentences for the prompt: '{prompt}'\\n\")\n",
    "for i in range(5):\n",
    "    generated_sentence = generate_text(model, tokenizer, prompt, max_sequence_length)\n",
    "    print(f\"Generated sentence {i + 1}: {generated_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc271d0d-9839-4d9e-bf09-2df18168e5cd",
   "metadata": {},
   "source": [
    "# 4 Compare Embedding Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560edac-b4ec-4465-98d8-9479a916ea3e",
   "metadata": {},
   "source": [
    "### Load the saved embeddings and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "85bc0213-3fcb-44c5-9b13-54c319442065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1562 RNN embeddings\n",
      "Loaded 1601 Word2Vec embeddings\n",
      "Loaded 1454 pre-trained embeddings (limited to 100000 lines)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import gzip\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Load RNN model and weights\n",
    "with open('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_architecture_rnn.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "rnn_model = model_from_json(loaded_model_json)\n",
    "rnn_model.load_weights('D:/6106_Neural_Modeling_Method/Assignments/Assignment7/rnn/model_weights_rnn.weights.h5')\n",
    "\n",
    "# Load RNN embeddings\n",
    "rnn_embeddings = {}\n",
    "with open(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_rnn_embeddings5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        if len(values) < 2:\n",
    "            continue\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            rnn_embeddings[word] = vector\n",
    "        except ValueError as e:\n",
    "            print(f\"Error on line {line_num}: {e}\")\n",
    "\n",
    "# Load Word2Vec embeddings\n",
    "word2vec_embeddings = {}\n",
    "with open(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/hindi_keras_embeddings5.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        if len(values) < 2:\n",
    "            continue\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec_embeddings[word] = vector\n",
    "        except ValueError as e:\n",
    "            print(f\"Error on line {line_num}: {e}\")\n",
    "\n",
    "# Specify target words if you only need embeddings for specific words\n",
    "target_words = set(rnn_embeddings.keys()).union(word2vec_embeddings.keys())\n",
    "\n",
    "# Limit the number of lines to load from the large pre-trained embeddings file\n",
    "max_lines = 100000  # Set a limit to prevent loading all data if not needed\n",
    "\n",
    "# Load pre-trained Hindi embeddings (gzip compressed)\n",
    "pretrained_embeddings = {}\n",
    "with gzip.open(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/cc.hi.300.vec.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        if line_num > max_lines:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word not in target_words:  # Only load target words if specified\n",
    "            continue\n",
    "        try:\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            pretrained_embeddings[word] = vector\n",
    "        except ValueError as e:\n",
    "            print(f\"Error on line {line_num}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(rnn_embeddings)} RNN embeddings\")\n",
    "print(f\"Loaded {len(word2vec_embeddings)} Word2Vec embeddings\")\n",
    "print(f\"Loaded {len(pretrained_embeddings)} pre-trained embeddings (limited to {max_lines} lines)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035e06c-914d-44e1-87cb-ab62f2f625d0",
   "metadata": {},
   "source": [
    "### Implement evaluation metric, Prepare evaluation data, Evaluate and compare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff2787-566c-4bb6-a38d-02209bcc27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_word_similarity(embeddings, word_pairs):\n",
    "    similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in embeddings and word2 in embeddings:\n",
    "            vec1 = embeddings[word1].reshape(1, -1)\n",
    "            vec2 = embeddings[word2].reshape(1, -1)\n",
    "            similarity = cosine_similarity(vec1, vec2)[0][0]\n",
    "            similarities.append(similarity)\n",
    "    return np.mean(similarities) if similarities else np.nan\n",
    "\n",
    "def evaluate_analogy_task(embeddings, analogies):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for a, b, c, d in analogies:\n",
    "        if a in embeddings and b in embeddings and c in embeddings and d in embeddings:\n",
    "            a_vec, b_vec, c_vec, d_vec = (embeddings[w] for w in (a, b, c, d))\n",
    "            result = b_vec - a_vec + c_vec\n",
    "            \n",
    "            max_similarity = -1\n",
    "            best_word = None\n",
    "            for word, vec in embeddings.items():\n",
    "                if word not in [a, b, c]:\n",
    "                    similarity = cosine_similarity(result.reshape(1, -1), vec.reshape(1, -1))[0][0]\n",
    "                    if similarity > max_similarity:\n",
    "                        max_similarity = similarity\n",
    "                        best_word = word\n",
    "            \n",
    "            if best_word == d:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def evaluate_chatbot_performance(embeddings, test_questions, test_answers):\n",
    "    correct = 0\n",
    "    total = len(test_questions)\n",
    "    \n",
    "    for question, correct_answer in zip(test_questions, test_answers):\n",
    "        question_words = question.split()\n",
    "        answer_words = correct_answer.split()\n",
    "        \n",
    "        question_embedding = np.mean([embeddings.get(word, np.zeros(embeddings[list(embeddings.keys())[0]].shape)) for word in question_words], axis=0)\n",
    "        answer_embedding = np.mean([embeddings.get(word, np.zeros(embeddings[list(embeddings.keys())[0]].shape)) for word in answer_words], axis=0)\n",
    "        \n",
    "        similarity = cosine_similarity(question_embedding.reshape(1, -1), answer_embedding.reshape(1, -1))[0][0]\n",
    "        \n",
    "        if similarity > 0.5:  # Adjust this threshold as needed\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Assuming you have already loaded your embeddings\n",
    "embedding_types = {\n",
    "    \"RNN\": rnn_embeddings,\n",
    "    \"Word2Vec\": word2vec_embeddings,\n",
    "    \"Pre-trained\": pretrained_embeddings\n",
    "}\n",
    "\n",
    "# Define your word pairs, analogies, test questions, and test answers\n",
    "word_pairs = [\n",
    "    (\"राजा\", \"रानी\"),  # king, queen\n",
    "    (\"मुंबई\", \"शहर\"),  # Mumbai, city\n",
    "    (\"खाना\", \"भोजन\"),  # food, meal\n",
    "    (\"सूरज\", \"चांद\")   # sun, moon\n",
    "]\n",
    "\n",
    "analogies = [\n",
    "    (\"राजा\", \"रानी\", \"लड़का\", \"लड़की\"),  # king, queen, boy, girl\n",
    "    (\"भारत\", \"दिल्ली\", \"फ्रांस\", \"पेरिस\"),  # India, Delhi, France, Paris\n",
    "    (\"गरम\", \"ठंडा\", \"दिन\", \"रात\"),  # hot, cold, day, night\n",
    "]\n",
    "\n",
    "test_questions = [\n",
    "    \"भारत की राजधानी क्या है?\",  # What is the capital of India?\n",
    "    \"सूरज किस दिशा में उगता है?\",  # In which direction does the sun rise?\n",
    "    \"पानी का रासायनिक सूत्र क्या है?\",  # What is the chemical formula for water?\n",
    "]\n",
    "\n",
    "test_answers = [\n",
    "    \"दिल्ली\",  # Delhi\n",
    "    \"पूरब\",  # East\n",
    "    \"H2O\",\n",
    "]\n",
    "\n",
    "print(\"Word Similarity Evaluation:\")\n",
    "for name, embeddings in embedding_types.items():\n",
    "    similarity_score = evaluate_word_similarity(embeddings, word_pairs)\n",
    "    print(f\"{name} Embeddings: {similarity_score:.4f}\")\n",
    "\n",
    "print(\"\\nAnalogy Task Evaluation:\")\n",
    "for name, embeddings in embedding_types.items():\n",
    "    analogy_score = evaluate_analogy_task(embeddings, analogies)\n",
    "    print(f\"{name} Embeddings: {analogy_score:.4f}\")\n",
    "\n",
    "print(\"\\nChatbot Performance Evaluation:\")\n",
    "for name, embeddings in embedding_types.items():\n",
    "    accuracy = evaluate_chatbot_performance(embeddings, test_questions, test_answers)\n",
    "    print(f\"{name} Embeddings: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4532c-0b48-4acd-aa37-cc47c563a113",
   "metadata": {},
   "source": [
    "## 5. Train and Compare a Classic DNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cd171-f266-4585-8271-28c9eb731d65",
   "metadata": {},
   "source": [
    "## Train a shallow DNN for embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fe17711a-bd04-4f44-a239-a187cff6fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Python310\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.0082 - loss: 7.2449 - val_accuracy: 0.0271 - val_loss: 7.1915 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0241 - loss: 6.5474 - val_accuracy: 0.0209 - val_loss: 7.4678 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0207 - loss: 6.4805 - val_accuracy: 0.0271 - val_loss: 7.7528 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0202 - loss: 6.4502 - val_accuracy: 0.0271 - val_loss: 7.8555 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.0220 - loss: 6.4276 - val_accuracy: 0.0271 - val_loss: 7.9813 - learning_rate: 5.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0177 - loss: 6.4171 - val_accuracy: 0.0271 - val_loss: 7.9117 - learning_rate: 5.0000e-04\n",
      "DNN embeddings saved in text format.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'sentences' and 'sequences' are defined and processed as in your setup\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert sentences to sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "# Prepare the input and target sequences for next word prediction\n",
    "input_sequences = []\n",
    "target_words = []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_seq = seq[:i+1]\n",
    "        input_sequences.append(n_gram_seq[:-1])\n",
    "        target_words.append(n_gram_seq[-1])\n",
    "\n",
    "# Pad the input sequences\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=padded_sequences.shape[1])\n",
    "target_words = np.array(target_words)\n",
    "\n",
    "# Define the improved DNN model\n",
    "embedding_dim = 300\n",
    "model = Sequential([\n",
    "    Embedding(total_words, embedding_dim, input_length=padded_sequences.shape[1]),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dropout(0.4),\n",
    "    Dense(64),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dropout(0.3),\n",
    "    Dense(32),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use a learning rate scheduler and early stopping for optimization\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Learning rate reduction and early stopping callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    padded_input_sequences, target_words,\n",
    "    epochs=50, batch_size=64, validation_split=0.2,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "# Extract DNN embeddings\n",
    "dnn_embeddings = model.layers[0].get_weights()[0]\n",
    "dnn_word_index = tokenizer.word_index\n",
    "\n",
    "# Save DNN embeddings in text format\n",
    "with open(\"D:/6106_Neural_Modeling_Method/Assignments/Assignment7/dnn_embeddings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, idx in dnn_word_index.items():\n",
    "        vector = dnn_embeddings[idx]\n",
    "        vector_str = \" \".join([str(v) for v in vector])\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n",
    "\n",
    "print(\"DNN embeddings saved in text format.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749888a7-0f15-4a9a-be42-d4738a5397cd",
   "metadata": {},
   "source": [
    "**we successfully generated language embeddings using three distinct methods: RNN-based embeddings, Keras's Word2Vec-based embeddings, and DNN-based embeddings. Each approach was applied to Hindi text data, allowing us to explore the unique qualities and effectiveness of different embedding techniques.\n",
    "We trained a chatbot using the RNN-based and Keras embeddings, both of which demonstrated satisfactory performance in understanding and responding to Hindi inputs. The RNN embeddings, with their ability to capture sequential dependencies, contributed to enhanced context comprehension, while Keras embeddings provided a reliable baseline performance.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46107a-cfa5-4f9f-8d22-a7971421332b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
